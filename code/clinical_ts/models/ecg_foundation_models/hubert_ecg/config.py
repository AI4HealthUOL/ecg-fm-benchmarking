from clinical_ts.models.ecg_foundation_models.hubert_ecg.hubert_ecg import HuBERTECGConfig

hubert_config = HuBERTECGConfig(
    activation_dropout = 0.1,
    apply_spec_augment = True,
    attention_dropout = 0.1,
    bos_token_id = 1,
    classifier_proj_size = 256,
    conv_bias = False,
    conv_dim = (512, 512, 512, 512, 512),
    conv_kernel = (10, 3, 3, 2, 2),
    conv_pos_batch_norm = False,
    conv_stride = (4, 2, 2, 2, 2),
    ctc_loss_reduction = "sum",
    ctc_zero_infinity = False,
    do_stable_layer_norm = False,
    ensemble_length = 1,
    eos_token_id = 2,
    feat_extract_activation = "gelu",
    feat_extract_norm = "group",
    feat_proj_dropout = 0.0,
    feat_proj_layer_norm = True,
    final_dropout = 0.1,
    hidden_act = "gelu",
    hidden_dropout = 0.1,
    hidden_size = 768,
    initializer_range = 0.02,
    intermediate_size = 3072,
    layer_norm_eps = 1e-05,
    layerdrop = 0.1,
    mask_feature_length = 10,
    mask_feature_min_masks = 0,
    mask_feature_prob = 0.0,
    mask_time_length = 1,
    mask_time_min_masks = 2,
    mask_time_prob = 0.33,
    model_type = "hubert_ecg",
    num_attention_heads = 12,
    num_conv_pos_embedding_groups = 16,
    num_conv_pos_embeddings = 128,
    num_feat_extract_layers = 5,
    num_hidden_layers = 12,
    pad_token_id = 0,
    transformers_version = "4.53.2",
    use_weighted_layer_sum = False,
    vocab_size = 100,
    vocab_sizes = (500)
)